"""
This script evaluates the best pre-trained transformer model on the test data.
"""

import time
import torch
from fairseq.models.transformer import TransformerModel
from fairseq.sequence_generator import SequenceGenerator
from fairseq.data import Dictionary
from sequence_utils import get_seq_nextword


def tokenize_input(model, input_sentence):
    """
    Tokenizes the input sentence using the model's dictionary.

    Args:
        model: The pre-trained model.
        input_sentence: The input sentence to tokenize.

    Returns:
        The tokenized input sentence.
    """
    tokens = model.encode(input_sentence)
    return tokens


def generate_predictions(model, generator, tokens):
    """
    Generates predictions for a single input sequence.

    Args:
        model: The pre-trained model.
        generator: The sequence generator.
        tokens: The tokenized input sequence.

    Returns:
        The generated predictions.
    """
    sample = {
        'net_input': {
            'src_tokens': tokens.unsqueeze(0),
            'src_lengths': torch.LongTensor([tokens.size(0)])
        }
    }
    hypos = generator.generate(model, sample)
    return hypos[0]


def parse_predictions(output, model):
    """
    Parses the top 3 predictions generated by the model.

    Args:
        output: The model's output.
        model: The pre-trained model.

    Returns:
        The decoded predictions.
    """
    predictions = [model.decode(pred['tokens']) for pred in output[:3]]
    return predictions


def calculate_accuracy(targets, predictions, top_k=1):
    """
    Calculates the top-K accuracy.

    Args:
        targets: The ground truth targets.
        predictions: The model's predictions.
        top_k: The top-K value for accuracy calculation.

    Returns:
        The top-K accuracy value.
    """
    correct = 0
    total = len(targets)

    for idx, ref in enumerate(targets):
        pred_list = predictions.get(idx, [])
        if ref in pred_list[:top_k]:
            correct += 1

    return correct / total


def main():
    """
    Main function to evaluate the pre-trained model on the test dataset.
    """
    model = TransformerModel.from_pretrained(
        'checkpoints/optuna_trial_34',               # Path to best trial
        checkpoint_file='checkpoint_best.pt',        # Best checkpoint file
        data_name_or_path='../../../data-bin/hrusso_dataset'
    )

    # Set the model to evaluation mode
    model.eval()

    # Load the dictionary used by the model
    src_dict = Dictionary.load('../data-bin/hrusso_dataset/dict.txt')

    # Create a custom sequence generator with an unknown token penalty
    generator = SequenceGenerator(
        model.models,  # Correctly access the list of models
        tgt_dict=src_dict,
        beam_size=3,
        max_len_b=1,  # Generate only the next word
        unk_penalty=10,  # High penalty to discourage <unk> token
        temperature=1.0,
    )

    inputs, targets = get_seq_nextword("../data/test_data.txt")
    tokenized_inputs = [tokenize_input(model, input_sentence) for input_sentence in inputs]

    # Generate and parse predictions for all test inputs
    all_predictions = {}
    start_time = time.time()
    for idx, tokens in enumerate(tokenized_inputs):
        output = generate_predictions(model, generator, tokens)
        all_predictions[idx] = parse_predictions(output, model)
    end_time = time.time()

    # Calculate top-1 and top-3 accuracy
    top_1_acc = calculate_accuracy(targets, all_predictions, top_k=1)
    top_3_acc = calculate_accuracy(targets, all_predictions, top_k=3)
    print(f'Top-1 Accuracy: {top_1_acc*100:.2f}%')
    print(f'Top-3 Accuracy: {top_3_acc*100:.2f}%')

    # Calculate average inference time
    inference_time = (end_time - start_time) / len(targets)
    print(f'Inference Time: {inference_time:.2f} s')


if __name__ == "__main__":
    main()
