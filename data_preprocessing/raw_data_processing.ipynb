{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all of the files of the raw data\n",
    "\n",
    "# These files are ELAN files that have been converted to tab-delimited text files\n",
    "# Due to the processing capabilities of the computer, the extraction to text files was done in 9 parts\n",
    "df1=pd.read_csv(\"../data/raw_data/Gusso_1.txt\", header=0, delimiter=\"\\t\", low_memory=False)\n",
    "df2=pd.read_csv(\"../data/raw_data/Gusso_2.txt\", header=0, delimiter=\"\\t\", low_memory=False)\n",
    "df3=pd.read_csv(\"../data/raw_data/Gusso_3.txt\", header=0, delimiter=\"\\t\", low_memory=False)\n",
    "df4=pd.read_csv(\"../data/raw_data/Gusso_4.txt\", header=0, delimiter=\"\\t\", low_memory=False)\n",
    "df5=pd.read_csv(\"../data/raw_data/Gusso_5.txt\", header=0, delimiter=\"\\t\", low_memory=False)\n",
    "df6=pd.read_csv(\"../data/raw_data/Gusso_6.txt\", header=0, delimiter=\"\\t\", low_memory=False)\n",
    "df7=pd.read_csv(\"../data/raw_data/Gusso_7.txt\", header=0, delimiter=\"\\t\", low_memory=False)\n",
    "df8=pd.read_csv(\"../data/raw_data/Gusso_8.txt\", header=0, delimiter=\"\\t\", low_memory=False)\n",
    "df9=pd.read_csv(\"../data/raw_data/Gusso_9.txt\", header=0, delimiter=\"\\t\", low_memory=False)\n",
    "\n",
    "full_df = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Begin Time - msec</th>\n",
       "      <th>End Time - msec</th>\n",
       "      <th>Duration - msec</th>\n",
       "      <th>B_phrase-gls-en</th>\n",
       "      <th>B_morph-hn-hru</th>\n",
       "      <th>B_word-txt-hru</th>\n",
       "      <th>A_morph-msa-en</th>\n",
       "      <th>A_word-txt-hru</th>\n",
       "      <th>interlinear-text-title-hru</th>\n",
       "      <th>B_phrase-segnum-en</th>\n",
       "      <th>...</th>\n",
       "      <th>C_word-txt-en</th>\n",
       "      <th>A_word-txt-en</th>\n",
       "      <th>E_morph-type</th>\n",
       "      <th>E_morph-cf-hru</th>\n",
       "      <th>E_phrase-gls-en</th>\n",
       "      <th>E_morph-msa-en</th>\n",
       "      <th>E_morph-hn-hru</th>\n",
       "      <th>E_morph-gls-en</th>\n",
       "      <th>E_phrase-segnum-en</th>\n",
       "      <th>E_word-txt-hru</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>603715</td>\n",
       "      <td>603715</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hru_1085_genesis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20971</td>\n",
       "      <td>21982</td>\n",
       "      <td>1011</td>\n",
       "      <td>Father Vijay.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fadar Vijay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hru_1085_genesis</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>515888</td>\n",
       "      <td>516454</td>\n",
       "      <td>566</td>\n",
       "      <td>Yes.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ẽ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hru_1085_genesis</td>\n",
       "      <td>174.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>603092</td>\n",
       "      <td>603715</td>\n",
       "      <td>623</td>\n",
       "      <td>Indeed.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>ã</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hru_1085_genesis</td>\n",
       "      <td>193.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12319</td>\n",
       "      <td>12934</td>\n",
       "      <td>615</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adv</td>\n",
       "      <td>yow</td>\n",
       "      <td>hru_1085_genesis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Begin Time - msec  End Time - msec  Duration - msec B_phrase-gls-en   \n",
       "0                  0           603715           603715             NaN  \\\n",
       "1              20971            21982             1011   Father Vijay.   \n",
       "2             515888           516454              566            Yes.   \n",
       "3             603092           603715              623         Indeed.   \n",
       "4              12319            12934              615             NaN   \n",
       "\n",
       "   B_morph-hn-hru B_word-txt-hru A_morph-msa-en A_word-txt-hru   \n",
       "0             NaN            NaN            NaN            NaN  \\\n",
       "1             NaN    Fadar Vijay            NaN            NaN   \n",
       "2             NaN              ẽ            NaN            NaN   \n",
       "3             2.0              ã            NaN            NaN   \n",
       "4             NaN            NaN            adv            yow   \n",
       "\n",
       "  interlinear-text-title-hru  B_phrase-segnum-en  ... C_word-txt-en   \n",
       "0           hru_1085_genesis                 NaN  ...           NaN  \\\n",
       "1           hru_1085_genesis                 6.0  ...           NaN   \n",
       "2           hru_1085_genesis               174.0  ...           NaN   \n",
       "3           hru_1085_genesis               193.0  ...           NaN   \n",
       "4           hru_1085_genesis                 NaN  ...           NaN   \n",
       "\n",
       "  A_word-txt-en E_morph-type E_morph-cf-hru E_phrase-gls-en E_morph-msa-en   \n",
       "0           NaN          NaN            NaN             NaN            NaN  \\\n",
       "1           NaN          NaN            NaN             NaN            NaN   \n",
       "2           NaN          NaN            NaN             NaN            NaN   \n",
       "3           NaN          NaN            NaN             NaN            NaN   \n",
       "4           NaN          NaN            NaN             NaN            NaN   \n",
       "\n",
       "  E_morph-hn-hru  E_morph-gls-en E_phrase-segnum-en E_word-txt-hru  \n",
       "0            NaN             NaN                NaN            NaN  \n",
       "1            NaN             NaN                NaN            NaN  \n",
       "2            NaN             NaN                NaN            NaN  \n",
       "3            NaN             NaN                NaN            NaN  \n",
       "4            NaN             NaN                NaN            NaN  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of the data extracted from the ELAN files\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of the columns that are speaker-dependent\n",
    "speaker_cols = full_df.filter(regex='^(A_|B_|C_|D_|E_)').columns\n",
    "# Get the names of the prev mentioned columns without the speaker specification\n",
    "unique_speaker_cols = speaker_cols.str.replace('[A|B|C|D|E]_', '', regex=True).unique()\n",
    "\n",
    "# Get the names of the columns that are not speaker-dependent\n",
    "general_cols = full_df.filter(regex='^(?!A_|B_|C_|D_|E_)').columns\n",
    "\n",
    "# Create a new dataframe, which will store the processed cleaned up data\n",
    "result_df = pd.DataFrame(columns=list(unique_speaker_cols) + list(general_cols))\n",
    "\n",
    "# The speaker-dependent columns are replaced with a general column, filled with the\n",
    "# non-NA value found in the relevant unprocessed columns\n",
    "for col in unique_speaker_cols:\n",
    "    original_cols = []\n",
    "    for letter in ['A', 'B', 'C', 'D', 'E']:\n",
    "        col_name = f'{letter}_{col}'\n",
    "        if col_name in speaker_cols:\n",
    "            original_cols.append(col_name)\n",
    "    result_df[col] = full_df[original_cols].bfill(axis=1).iloc[:, 0]\n",
    "\n",
    "# Copy the columns that are not speaker-dependent to new dataframe\n",
    "for col in general_cols:\n",
    "    result_df[col] = full_df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = result_df.drop(columns=[\"File Path\"])  # Removes the \"File Path\" column\n",
    "\n",
    "# Generalisation of the \"interlinear-text-title\" (it was called differently in different files)\n",
    "result_df['interlinear-text-title-hru'] = result_df['interlinear-text-title-hru'].fillna('')\n",
    "result_df['interlinear-text-title-en'] = result_df['interlinear-text-title-en'].fillna('')\n",
    "result_df['interlinear-text-title'] = result_df['interlinear-text-title-hru'] + result_df['interlinear-text-title-en']\n",
    "result_df = result_df.drop(columns=[\"interlinear-text-title-hru\", \"interlinear-text-title-en\"])\n",
    "\n",
    "# Save dataframe to csv file\n",
    "result_df.to_csv(\"../data/data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data as Split Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/data.csv\", index_col=0, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different sentences can be identified based on the phrase number and the text title\n",
    "sent_id = df[['phrase-segnum-en', 'interlinear-text-title', 'phrase-gls-en']].drop_duplicates()\n",
    "\n",
    "# Create empty list of sentences\n",
    "sentences = []\n",
    "\n",
    "for index, row in sent_id.iterrows():\n",
    "    # Get the df of words for each sentence\n",
    "    hru_word_df = df.loc[(df['phrase-segnum-en'] == row['phrase-segnum-en']) & (df['interlinear-text-title'] == row['interlinear-text-title']), 'word-txt-hru']\n",
    "    # Concatenate words into sentence\n",
    "    sentence_raw = hru_word_df.str.cat(sep=' ')\n",
    "\n",
    "    # Append non-empty, non-filler sentences without duplicate words\n",
    "    if sentence_raw not in [\"\", \"FILLER\", \"filler\", \"LAUGH\", \"INDISTINCT\"]:\n",
    "        # Remove duplicate words, the pattern is case-insensitive and matches sequences of duplicates\n",
    "        sentence = re.sub(r'\\b(\\w+)(\\s+\\1)+\\b', r'\\1', sentence_raw, flags=re.IGNORECASE).split()\n",
    "        # Remove filler words\n",
    "        for fil in [\"FILLER\", \"filler\", \"LAUGH\", \"INDISTINCT\"]:\n",
    "            while fil in sentence:\n",
    "                sentence.remove(fil)\n",
    "        sentences.append(sentence)\n",
    "\n",
    "np.save(\"../data/split_sentences.npy\", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train and Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/split_sentences.txt', 'r') as file:\n",
    "    sentences = file.readlines()\n",
    "\n",
    "# Strip newline characters from the end of each line\n",
    "sentences = [sentence.strip() for sentence in sentences]\n",
    "random.shuffle(sentences)\n",
    "\n",
    "# Create data splits\n",
    "train_end = int(0.8 * len(sentences))\n",
    "val_end = train_end + int(0.1 * len(sentences))\n",
    "train = sentences[:train_end]\n",
    "val = sentences[train_end:val_end]\n",
    "test = sentences[val_end:]\n",
    "\n",
    "# Write train, validation and test data to files\n",
    "with open(\"../data/train_data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(train))\n",
    "\n",
    "with open(\"../data/val_data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(val))\n",
    "\n",
    "with open(\"../data/test_data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
