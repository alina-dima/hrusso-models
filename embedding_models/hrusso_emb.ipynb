{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Hrusso Aka Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.stats import spearmanr\n",
    "from keybert import KeyBERT\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bilingual Hrusso dictionary digitalised from a NEILAC resource\n",
    "# The file contains three columns: the Hrusso Aka words, the part-of-speech, the English translation\n",
    "bidict_df = pd.read_csv(\"../data/hrusso_dict.csv\", names=['word', 'pos', 'eng'])\n",
    "\n",
    "with open('../data/train_data.txt', 'r') as file:\n",
    "    sentences = file.readlines()\n",
    "\n",
    "# Strip newline characters from the end of each line\n",
    "sentences = [sentence.strip().split() for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of unique words\n",
    "unique_words = list(set([word for sentence in sentences for word in sentence]))\n",
    "with open('unique_words.pkl', 'wb') as f:\n",
    "    pkl.dump(unique_words, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\"w2v\", \"svd\", \"clwe-reg\", \"clwe-orth\"]\n",
    "clwe_base = [\"w2v\", \"svd\"]\n",
    "w2v_algs = [\"cbow\", \"sg\"]\n",
    "dims = [50, 100, 200]\n",
    "win_sizes = [2, 5, 10]\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_co_mat(sentences, win_size, unique_words):\n",
    "    # Create and initialize co-occurrence matrix\n",
    "    matrix_size = len(unique_words)\n",
    "    co_mat = np.zeros((matrix_size, matrix_size))\n",
    "\n",
    "    # Fill co-occurrence matrix\n",
    "    for sentence in sentences:\n",
    "        for target_id, target in enumerate(sentence):\n",
    "            context_win = sentence[max(target_id - win_size, 0) : min(target_id + win_size, len(sentence) + 1)]\n",
    "            # Iterate through the context window and update the co-occurrence matrix\n",
    "            for context in context_win:\n",
    "                if context != target:\n",
    "                    context_id = unique_words.index(context)\n",
    "                    co_mat[target_id, context_id] += 1\n",
    "    \n",
    "    return co_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLWE Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seed_lexicon(sentences, no_seed_words = 100):\n",
    "    kw_model = KeyBERT()\n",
    "    seed_lexicon = []\n",
    "    flattened_sent = [word for sentence in sentences for word in sentence]\n",
    "    word_freq = Counter(flattened_sent)\n",
    "\n",
    "    # Get the most common words that are present in the dictionary\n",
    "    for word, _ in word_freq.most_common():\n",
    "        if len(seed_lexicon) >= no_seed_words:\n",
    "            break\n",
    "        if word in bidict_df['word'].values:\n",
    "            eng_word = bidict_df.loc[bidict_df['word'] == word, 'eng'].values[0]\n",
    "            if len(eng_word.split()) > 1:\n",
    "                eng_word = kw_model.extract_keywords(eng_word, keyphrase_ngram_range=(1, 1), stop_words=None)[0][0]\n",
    "            seed_lexicon.append((word, eng_word))\n",
    "    \n",
    "    return seed_lexicon\n",
    "\n",
    "def get_embedding_matrices(source_we_model, target_we_model, seed_lexicon, is_hru_svd = False):\n",
    "    source_we = []\n",
    "    target_we = []\n",
    "\n",
    "    for word, eng_word in seed_lexicon:\n",
    "        if is_hru_svd:\n",
    "            if word in unique_words and eng_word in target_we_model.index_to_key:\n",
    "                source_we.append(source_we_model.components_[:, unique_words.index(word)])\n",
    "                target_we.append(target_we_model.get_vector(eng_word))\n",
    "        else:\n",
    "            if word in source_we_model.wv and eng_word in target_we_model.index_to_key:\n",
    "                source_we.append(source_we_model.wv[word])\n",
    "                target_we.append(target_we_model.get_vector(eng_word))\n",
    "            else:\n",
    "                print(f\"Word {word} or {eng_word} not in vocabulary\")\n",
    "    \n",
    "    # Normalize the embedding matrices\n",
    "    source_we, target_we = np.array(source_we), np.array(target_we)\n",
    "    normalized_source_we = source_we / np.linalg.norm(source_we, axis=1, keepdims=True)\n",
    "    normalized_target_we = target_we / np.linalg.norm(target_we, axis=1, keepdims=True)\n",
    "\n",
    "    return normalized_source_we, normalized_target_we\n",
    "\n",
    "def learn_transformation_matrix(Xs, Xt, learning_rate=0.01, epochs=100):\n",
    "    # Initialize W\n",
    "    W = np.random.randn(Xs.shape[1], Xt.shape[1])\n",
    "\n",
    "    def mse_loss(W, Xs, Xt):\n",
    "        transformed_Xs = np.dot(Xs, W)\n",
    "        mse = np.mean(np.linalg.norm(transformed_Xs - Xt, axis=1) ** 2)\n",
    "        return mse\n",
    "\n",
    "    # Gradient descent\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle indices\n",
    "        indices = np.random.permutation(len(Xs))\n",
    "\n",
    "        for idx in indices:\n",
    "            xs_sample = Xs[idx:idx+1]\n",
    "            xt_sample = Xt[idx:idx+1]\n",
    "\n",
    "            gradient = np.dot(xs_sample.T, np.dot(xs_sample, W) - xt_sample)\n",
    "\n",
    "            # Update W\n",
    "            W -= learning_rate * gradient\n",
    "\n",
    "        # Calculate and print MSE loss\n",
    "        loss = mse_loss(W, Xs, Xt)\n",
    "        # if (epoch+1) % 10 == 0:\n",
    "        #     print(f'Epoch {epoch+1}/{epochs}, Loss: {loss}')\n",
    "\n",
    "    return W\n",
    "\n",
    "def learn_orthogonal_transformation_matrix(Xs, Xt):\n",
    "    product = np.dot(Xt.T, Xs)\n",
    "\n",
    "    # Compute SVD of the product\n",
    "    U, _, Vt = np.linalg.svd(product)\n",
    "\n",
    "    # Construct orthogonal matrix W using U and V\n",
    "    W = np.dot(Vt.T, U.T)\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in methods:\n",
    "    if method == \"w2v\":\n",
    "        for dim in dims:\n",
    "            for win_size in win_sizes:\n",
    "                for idx, algorithm in enumerate(w2v_algs):\n",
    "                    model = Word2Vec(sentences, vector_size=dim, window=win_size, min_count=1,\\\n",
    "                                     sample=1e-3, sg=idx, hs=0, negative=5, seed=seed)\n",
    "                    model.save(f\"emb_models/{method}_{dim}_{win_size}_{algorithm}.model\")\n",
    "    \n",
    "    elif method == \"svd\":\n",
    "        for win_size in win_sizes:\n",
    "            co_mat = get_co_mat(sentences, win_size, unique_words)\n",
    "            for dim in dims:\n",
    "                model = TruncatedSVD(n_components=dim)\n",
    "                model.fit_transform(co_mat)\n",
    "                with open(f\"emb_models/{method}_{dim}_{win_size}.pkl\", 'wb') as f:\n",
    "                    pkl.dump(model, f)\n",
    "\n",
    "    elif method == \"clwe-reg\":\n",
    "        eng_model = gensim.downloader.load(\"glove-wiki-gigaword-100\")\n",
    "        for base in clwe_base:\n",
    "            if base == \"w2v\":\n",
    "                hrusso_model = Word2Vec.load(f\"emb_models/w2v_100_10_sg.model\")\n",
    "                Xs, Xt = get_embedding_matrices(hrusso_model, eng_model, get_seed_lexicon(sentences))\n",
    "                W = learn_transformation_matrix(Xs, Xt, epochs=500)\n",
    "                hrusso_model.wv.vectors = np.dot(hrusso_model.wv.vectors, W)\n",
    "                hrusso_model.save(f\"emb_models/{method}_{base}_100_10_sg.model\")\n",
    "            else:\n",
    "                hrusso_model = []\n",
    "                with open(f\"emb_models/svd_100_5.pkl\", 'rb') as f:\n",
    "                    hrusso_model = pkl.load(f)\n",
    "                # eng_model = gensim.downloader.load(\"glove-wiki-gigaword-100\")\n",
    "                Xs, Xt = get_embedding_matrices(hrusso_model, eng_model, get_seed_lexicon(sentences), is_hru_svd=True)\n",
    "                W = learn_transformation_matrix(Xs, Xt, epochs=500)\n",
    "                hrusso_model.components_ = np.dot(hrusso_model.components_.T, W).T\n",
    "                with open(f\"emb_models/{method}_{base}_100_5.pkl\", 'wb') as f:\n",
    "                    pkl.dump(hrusso_model, f)\n",
    "\n",
    "    if method == \"clwe-orth\":\n",
    "        # eng_model = gensim.downloader.load(\"glove-wiki-gigaword-100\")\n",
    "        for base in clwe_base:\n",
    "            if base == \"w2v\":\n",
    "                hrusso_model = Word2Vec.load(f\"emb_models/w2v_100_10_sg.model\")\n",
    "                Xs, Xt = get_embedding_matrices(hrusso_model, eng_model, get_seed_lexicon(sentences))\n",
    "                W = learn_orthogonal_transformation_matrix(Xs, Xt)\n",
    "                hrusso_model.wv.vectors = np.dot(hrusso_model.wv.vectors, W)\n",
    "                hrusso_model.save(f\"emb_models/{method}_{base}_100_10_sg.model\")\n",
    "            else:\n",
    "                hrusso_model = []\n",
    "                with open(f\"emb_models/svd_100_5.pkl\", 'rb') as f:\n",
    "                    hrusso_model = pkl.load(f)\n",
    "                eng_model = gensim.downloader.load(\"glove-wiki-gigaword-100\")\n",
    "                Xs, Xt = get_embedding_matrices(hrusso_model, eng_model, get_seed_lexicon(sentences), is_hru_svd=True)\n",
    "                W = learn_orthogonal_transformation_matrix(Xs, Xt)\n",
    "                hrusso_model.components_ = np.dot(hrusso_model.components_.T, W).T\n",
    "                with open(f\"emb_models/{method}_{base}_100_5.pkl\", 'wb') as f:\n",
    "                    pkl.dump(hrusso_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypernym-Hyponym Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_eval_data_file = \"valid_emb_eval_pairs.csv\"\n",
    "\n",
    "# Load the evaluation data, where the hypernym is the key and the hyponyms are the value\n",
    "emb_eval_data = {}\n",
    "with open(emb_eval_data_file, 'r', newline='') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "    for row in csv_reader:\n",
    "        key = row[0]\n",
    "        values = row[1:]\n",
    "        emb_eval_data[key] = values\n",
    "\n",
    "# Check if all words in the evaluation data exist in the sentences\n",
    "for key in emb_eval_data.keys():\n",
    "    for value in emb_eval_data[key]:\n",
    "        found = False\n",
    "        for sentence in sentences:\n",
    "            if value in sentence:\n",
    "                found = True\n",
    "                break\n",
    "        # If the word is not found in any sentence, print it\n",
    "        if not found:\n",
    "            print(value)\n",
    "    found = False\n",
    "    for sentence in sentences:\n",
    "        if key in sentence:\n",
    "            found = True\n",
    "            break\n",
    "    # If the word is not found in any sentence, print it\n",
    "    if not found:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Gold Standard\n",
    "The gold standard vectors have similarity scores of 1 for the hyponyms of their corresponding hypernym and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyponyms = [value for values in emb_eval_data.values() for value in values]\n",
    "\n",
    "gold_standard = []\n",
    "for key in emb_eval_data.keys():\n",
    "    gold_standard.append([0 for _ in range(len(hyponyms))])\n",
    "    for value in emb_eval_data[key]:\n",
    "        gold_standard[-1][hyponyms.index(value)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with the Similarity Scores from the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_w2v(filename, emb_eval_data, hyponyms, gold_standard):\n",
    "    model = Word2Vec.load(filename)\n",
    "    model_sim = []\n",
    "    for key in emb_eval_data.keys():\n",
    "        similarities = []\n",
    "        for value in hyponyms:\n",
    "            similarities.append(model.wv.similarity(key, value))\n",
    "        model_sim.append(similarities)\n",
    "    correlation = spearmanr(np.array(model_sim).flatten(), np.array(gold_standard).flatten())\n",
    "    new_eval_value = (correlation[0], correlation[1])\n",
    "\n",
    "    return new_eval_value\n",
    "\n",
    "def evaluate_svd(filename, emb_eval_data, unique_words, hyponyms, gold_standard):\n",
    "    model = pkl.load(filename)\n",
    "    model_sim = []\n",
    "    for key in emb_eval_data.keys():\n",
    "        similarities = []\n",
    "        for value in hyponyms:\n",
    "            key_vec = model.components_[:, unique_words.index(key)]\n",
    "            value_vec = model.components_[:, unique_words.index(value)]\n",
    "            similarities.append(np.dot(key_vec,value_vec)/(norm(key_vec)*norm(value_vec)))\n",
    "        model_sim.append(similarities)\n",
    "    correlation = spearmanr(np.array(model_sim).flatten(), np.array(gold_standard).flatten())\n",
    "    new_eval_value = (correlation[0], correlation[1])\n",
    "\n",
    "    return new_eval_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(columns=[\"method\", \"dim\", \"win_size = 2\", \"win_size = 5\", \"win_size = 10\"])\n",
    "\n",
    "for method in methods:\n",
    "    if method not in [\"clwe-reg\", \"clwe-orth\"]:\n",
    "        for dim in dims:\n",
    "            if method == \"w2v\":\n",
    "                for alg in w2v_algs:\n",
    "                    eval_df.loc[len(eval_df)] = [method+'-'+alg, dim, (0.0, 0.0), (0.0, 0.0), (0.0, 0.0)]\n",
    "            else:\n",
    "                eval_df.loc[len(eval_df)] = [method, dim, (0.0, 0.0), (0.0, 0.0), (0.0, 0.0)]\n",
    "\n",
    "            for win_size in win_sizes:\n",
    "                if method == \"w2v\":\n",
    "                    for alg in w2v_algs:\n",
    "                        filename = f\"emb_models/{method}_{dim}_{win_size}_{alg}.model\"\n",
    "                        new_eval_value = evaluate_w2v(filename, emb_eval_data, hyponyms, gold_standard)\n",
    "                        if alg == \"cbow\":\n",
    "                            eval_df.at[len(eval_df) - 2, f\"win_size = {win_size}\"] = new_eval_value\n",
    "                        else:\n",
    "                            eval_df.at[len(eval_df) - 1, f\"win_size = {win_size}\"] = new_eval_value\n",
    "\n",
    "                if method == \"svd\":\n",
    "                    with open(f\"emb_models/{method}_{dim}_{win_size}.pkl\", 'rb') as f:\n",
    "                        new_eval_value = evaluate_svd(f, emb_eval_data, unique_words, hyponyms, gold_standard)\n",
    "                        eval_df.at[len(eval_df) - 1, f\"win_size = {win_size}\"] = new_eval_value\n",
    "    \n",
    "    elif method == \"clwe-reg\" or method == \"clwe-orth\":\n",
    "        for base in clwe_base:\n",
    "            if base == \"w2v\":\n",
    "                eval_df.loc[len(eval_df)] = [method+'-'+base, 100, ('NA', 'NA'), ('NA', 'NA'), (0.0, 0.0)]\n",
    "                filename = f\"emb_models/{method}_{base}_100_10_sg.model\"\n",
    "                new_eval_value = evaluate_w2v(filename, emb_eval_data, hyponyms, gold_standard)\n",
    "                eval_df.at[len(eval_df) - 1, f\"win_size = {10}\"] = new_eval_value\n",
    "\n",
    "            else:\n",
    "                eval_df.loc[len(eval_df)] = [method+'-'+base, 100, ('NA', 'NA'), (0.0, 0.0), ('NA', 'NA')]\n",
    "                with open(f\"emb_models/{method}_{base}_100_5.pkl\", 'rb') as f:\n",
    "                        new_eval_value = evaluate_svd(f, emb_eval_data, unique_words, hyponyms, gold_standard)\n",
    "                        eval_df.at[len(eval_df) - 1, f\"win_size = {5}\"] = new_eval_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>dim</th>\n",
       "      <th>win_size = 2</th>\n",
       "      <th>win_size = 5</th>\n",
       "      <th>win_size = 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>w2v-cbow</td>\n",
       "      <td>50</td>\n",
       "      <td>(0.010152260353641482, 0.7546531507406037)</td>\n",
       "      <td>(0.002238703565161967, 0.9450606871145031)</td>\n",
       "      <td>(0.007097905101947633, 0.8270498048380326)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>w2v-sg</td>\n",
       "      <td>50</td>\n",
       "      <td>(0.004043549850253786, 0.9009455285831092)</td>\n",
       "      <td>(0.014907336195371934, 0.646309195959095)</td>\n",
       "      <td>(0.030127049528071128, 0.35363272848306226)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>w2v-cbow</td>\n",
       "      <td>100</td>\n",
       "      <td>(0.012998364110901658, 0.6890637744038759)</td>\n",
       "      <td>(0.007375573761192528, 0.8203971187212318)</td>\n",
       "      <td>(0.004616241508403861, 0.8870045336497803)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>w2v-sg</td>\n",
       "      <td>100</td>\n",
       "      <td>(0.007670596738480087, 0.8133427588175175)</td>\n",
       "      <td>(0.020668960894863767, 0.5245878217296382)</td>\n",
       "      <td>(0.025562870941733165, 0.43128663050119087)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>w2v-cbow</td>\n",
       "      <td>200</td>\n",
       "      <td>(0.01084643200175372, 0.738470746092208)</td>\n",
       "      <td>(0.007826785332465483, 0.8096141716256643)</td>\n",
       "      <td>(0.009822528889527348, 0.7623781244596826)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>w2v-sg</td>\n",
       "      <td>200</td>\n",
       "      <td>(0.013119844149321295, 0.6863114549863787)</td>\n",
       "      <td>(0.027992471710125998, 0.3887871995343488)</td>\n",
       "      <td>(0.023601836035816087, 0.46747047984395595)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>svd</td>\n",
       "      <td>50</td>\n",
       "      <td>(0.02727227492340868, 0.40111170399743046)</td>\n",
       "      <td>(0.03361526205983512, 0.3006593994280169)</td>\n",
       "      <td>(0.018204651471743438, 0.5751979445275196)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svd</td>\n",
       "      <td>100</td>\n",
       "      <td>(0.02015701374601437, 0.5349115665452605)</td>\n",
       "      <td>(0.03425737083433895, 0.29151564983428385)</td>\n",
       "      <td>(0.016729536719504935, 0.6065557391892258)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>svd</td>\n",
       "      <td>200</td>\n",
       "      <td>(0.009076296712712378, 0.7799473156443644)</td>\n",
       "      <td>(0.02648264837548188, 0.4148900950430032)</td>\n",
       "      <td>(0.0061260647945905, 0.8504289287301849)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>clwe-reg-w2v</td>\n",
       "      <td>100</td>\n",
       "      <td>(NA, NA)</td>\n",
       "      <td>(NA, NA)</td>\n",
       "      <td>(0.04060904113037922, 0.2111086873581284)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>clwe-reg-svd</td>\n",
       "      <td>100</td>\n",
       "      <td>(NA, NA)</td>\n",
       "      <td>(0.05090013609782985, 0.11692819664919034)</td>\n",
       "      <td>(NA, NA)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>clwe-orth-w2v</td>\n",
       "      <td>100</td>\n",
       "      <td>(NA, NA)</td>\n",
       "      <td>(NA, NA)</td>\n",
       "      <td>(0.025562870941733165, 0.43128663050119087)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>clwe-orth-svd</td>\n",
       "      <td>100</td>\n",
       "      <td>(NA, NA)</td>\n",
       "      <td>(0.03425737083433895, 0.29151564983428385)</td>\n",
       "      <td>(NA, NA)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           method  dim                                win_size = 2  \\\n",
       "0        w2v-cbow   50  (0.010152260353641482, 0.7546531507406037)   \n",
       "1          w2v-sg   50  (0.004043549850253786, 0.9009455285831092)   \n",
       "2        w2v-cbow  100  (0.012998364110901658, 0.6890637744038759)   \n",
       "3          w2v-sg  100  (0.007670596738480087, 0.8133427588175175)   \n",
       "4        w2v-cbow  200    (0.01084643200175372, 0.738470746092208)   \n",
       "5          w2v-sg  200  (0.013119844149321295, 0.6863114549863787)   \n",
       "6             svd   50  (0.02727227492340868, 0.40111170399743046)   \n",
       "7             svd  100   (0.02015701374601437, 0.5349115665452605)   \n",
       "8             svd  200  (0.009076296712712378, 0.7799473156443644)   \n",
       "9    clwe-reg-w2v  100                                    (NA, NA)   \n",
       "10   clwe-reg-svd  100                                    (NA, NA)   \n",
       "11  clwe-orth-w2v  100                                    (NA, NA)   \n",
       "12  clwe-orth-svd  100                                    (NA, NA)   \n",
       "\n",
       "                                  win_size = 5  \\\n",
       "0   (0.002238703565161967, 0.9450606871145031)   \n",
       "1    (0.014907336195371934, 0.646309195959095)   \n",
       "2   (0.007375573761192528, 0.8203971187212318)   \n",
       "3   (0.020668960894863767, 0.5245878217296382)   \n",
       "4   (0.007826785332465483, 0.8096141716256643)   \n",
       "5   (0.027992471710125998, 0.3887871995343488)   \n",
       "6    (0.03361526205983512, 0.3006593994280169)   \n",
       "7   (0.03425737083433895, 0.29151564983428385)   \n",
       "8    (0.02648264837548188, 0.4148900950430032)   \n",
       "9                                     (NA, NA)   \n",
       "10  (0.05090013609782985, 0.11692819664919034)   \n",
       "11                                    (NA, NA)   \n",
       "12  (0.03425737083433895, 0.29151564983428385)   \n",
       "\n",
       "                                  win_size = 10  \n",
       "0    (0.007097905101947633, 0.8270498048380326)  \n",
       "1   (0.030127049528071128, 0.35363272848306226)  \n",
       "2    (0.004616241508403861, 0.8870045336497803)  \n",
       "3   (0.025562870941733165, 0.43128663050119087)  \n",
       "4    (0.009822528889527348, 0.7623781244596826)  \n",
       "5   (0.023601836035816087, 0.46747047984395595)  \n",
       "6    (0.018204651471743438, 0.5751979445275196)  \n",
       "7    (0.016729536719504935, 0.6065557391892258)  \n",
       "8      (0.0061260647945905, 0.8504289287301849)  \n",
       "9     (0.04060904113037922, 0.2111086873581284)  \n",
       "10                                     (NA, NA)  \n",
       "11  (0.025562870941733165, 0.43128663050119087)  \n",
       "12                                     (NA, NA)  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
